{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPxSvzot97M"
      },
      "source": [
        "First we import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qZTxvI2shhJ",
        "outputId": "f6431009-6381-4b42-c278-45d28a159b95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1) (1.4.1)\n",
            "Requirement already satisfied: costcla in /usr/local/lib/python3.7/dist-packages (0.6)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0b2 in /usr/local/lib/python3.7/dist-packages (from costcla) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from costcla) (1.3.5)\n",
            "Requirement already satisfied: pyea>=0.2 in /usr/local/lib/python3.7/dist-packages (from costcla) (0.2)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.7/dist-packages (from costcla) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.14.0->costcla) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.14.0->costcla) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15.0b2->costcla) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.15.0b2->costcla) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==0.22.2.post1\n",
        "!pip install costcla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTspk66PB_1t",
        "outputId": "a9ae4536-c706-41ae-dc4e-c52814b35896"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from costcla.metrics import cost_loss\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from costcla.models import BayesMinimumRiskClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogT3pPeZ8S6X"
      },
      "source": [
        "We take the file and make a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BoaCXOeiuEB5"
      },
      "outputs": [],
      "source": [
        "heartArray = np.loadtxt('heart.dat', unpack = True)\n",
        "columns = ['age ','sex','cpt', 'rbp', 'sc', 'fbs', \n",
        "                                         'rer', 'mhr', 'eia', 'oldpeak', 'slope', \n",
        "                                         'nmv', 'thal', 'presence']\n",
        "df = pd.DataFrame(heartArray.transpose(1, 0), columns = columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WX516Q04XKC",
        "outputId": "dfd50f94-0e40-438a-92e3-261809c20b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     age   sex  cpt    rbp     sc  fbs  rer    mhr  eia  oldpeak  slope  nmv  \\\n",
            "0    70.0  1.0  4.0  130.0  322.0  0.0  2.0  109.0  0.0      2.4    2.0  3.0   \n",
            "1    67.0  0.0  3.0  115.0  564.0  0.0  2.0  160.0  0.0      1.6    2.0  0.0   \n",
            "2    57.0  1.0  2.0  124.0  261.0  0.0  0.0  141.0  0.0      0.3    1.0  0.0   \n",
            "3    64.0  1.0  4.0  128.0  263.0  0.0  0.0  105.0  1.0      0.2    2.0  1.0   \n",
            "4    74.0  0.0  2.0  120.0  269.0  0.0  2.0  121.0  1.0      0.2    1.0  1.0   \n",
            "..    ...  ...  ...    ...    ...  ...  ...    ...  ...      ...    ...  ...   \n",
            "265  52.0  1.0  3.0  172.0  199.0  1.0  0.0  162.0  0.0      0.5    1.0  0.0   \n",
            "266  44.0  1.0  2.0  120.0  263.0  0.0  0.0  173.0  0.0      0.0    1.0  0.0   \n",
            "267  56.0  0.0  2.0  140.0  294.0  0.0  2.0  153.0  0.0      1.3    2.0  0.0   \n",
            "268  57.0  1.0  4.0  140.0  192.0  0.0  0.0  148.0  0.0      0.4    2.0  0.0   \n",
            "269  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0      1.5    2.0  3.0   \n",
            "\n",
            "     thal  presence  \n",
            "0     3.0       2.0  \n",
            "1     7.0       1.0  \n",
            "2     7.0       2.0  \n",
            "3     7.0       1.0  \n",
            "4     3.0       1.0  \n",
            "..    ...       ...  \n",
            "265   7.0       1.0  \n",
            "266   7.0       1.0  \n",
            "267   3.0       1.0  \n",
            "268   6.0       1.0  \n",
            "269   3.0       2.0  \n",
            "\n",
            "[270 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zLIql8w8bVd"
      },
      "source": [
        "Now we have to check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "vNfi2Ih18W2L",
        "outputId": "deaa5d45-2550-40bd-ce92-500a9040f53a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With heart disease: 120\n",
            "Without heart disease: 150\n",
            "Percentages with and without heart disease: 44.44444444444444 55.55555555555556\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with_hd = (df.presence == 2)\n",
        "without_hd = (df.presence == 1)\n",
        "print(\"With heart disease:\",with_hd.sum())\n",
        "print(\"Without heart disease:\",without_hd.sum())\n",
        "\n",
        "df['presence'].value_counts().plot(kind='pie')\n",
        "print(\"Percentages with and without heart disease:\",with_hd.sum()*100/len(df),without_hd.sum()*100/len(df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed29bawI_WxF"
      },
      "source": [
        "We have about half from each type which is very good for classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlD2wrwo_oW6"
      },
      "source": [
        "Now we have to drop unnecessary columns.\n",
        "Attribute Information:\n",
        "------------------------\n",
        "1. age\n",
        "2. sex\n",
        "3. chest pain type (4 values) [cpt]\n",
        "4. resting blood pressure [rbp]\n",
        "5. serum cholesterol in mg/dl [sc]\n",
        "6. fasting blood sugar > 120 mg/dl [fbs]\n",
        "7. resting electrocardiographic results (values 0,1,2) [rer]\n",
        "8. maximum heart rate achieved [mhr]\n",
        "9. exercise induced angina [eia]\n",
        "10. oldpeak = ST depression induced by exercise relative to rest [oldpeak]\n",
        "11. the slope of the peak exercise ST segment [slope]\n",
        "12. number of major vessels (0-3) colored by flourosopy [nmv]\n",
        "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect [thal] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "8Sv53UfN_msZ",
        "outputId": "1ab399da-5330-4714-d4b1-3902a11ff689"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b5606850-4a47-431d-a587-f666860b588c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cpt</th>\n",
              "      <th>rbp</th>\n",
              "      <th>sc</th>\n",
              "      <th>fbs</th>\n",
              "      <th>rer</th>\n",
              "      <th>mhr</th>\n",
              "      <th>eia</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>nmv</th>\n",
              "      <th>thal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.00000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "      <td>270.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>54.433333</td>\n",
              "      <td>0.677778</td>\n",
              "      <td>3.174074</td>\n",
              "      <td>131.344444</td>\n",
              "      <td>249.659259</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>1.022222</td>\n",
              "      <td>149.677778</td>\n",
              "      <td>0.329630</td>\n",
              "      <td>1.05000</td>\n",
              "      <td>1.585185</td>\n",
              "      <td>0.670370</td>\n",
              "      <td>4.696296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>9.109067</td>\n",
              "      <td>0.468195</td>\n",
              "      <td>0.950090</td>\n",
              "      <td>17.861608</td>\n",
              "      <td>51.686237</td>\n",
              "      <td>0.355906</td>\n",
              "      <td>0.997891</td>\n",
              "      <td>23.165717</td>\n",
              "      <td>0.470952</td>\n",
              "      <td>1.14521</td>\n",
              "      <td>0.614390</td>\n",
              "      <td>0.943896</td>\n",
              "      <td>1.940659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>126.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>48.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>120.000000</td>\n",
              "      <td>213.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>55.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>245.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>153.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.80000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>61.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>280.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.60000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>564.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.20000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5606850-4a47-431d-a587-f666860b588c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b5606850-4a47-431d-a587-f666860b588c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b5606850-4a47-431d-a587-f666860b588c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             age          sex         cpt         rbp          sc         fbs  \\\n",
              "count  270.000000  270.000000  270.000000  270.000000  270.000000  270.000000   \n",
              "mean    54.433333    0.677778    3.174074  131.344444  249.659259    0.148148   \n",
              "std      9.109067    0.468195    0.950090   17.861608   51.686237    0.355906   \n",
              "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
              "25%     48.000000    0.000000    3.000000  120.000000  213.000000    0.000000   \n",
              "50%     55.000000    1.000000    3.000000  130.000000  245.000000    0.000000   \n",
              "75%     61.000000    1.000000    4.000000  140.000000  280.000000    0.000000   \n",
              "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
              "\n",
              "              rer         mhr         eia    oldpeak       slope         nmv  \\\n",
              "count  270.000000  270.000000  270.000000  270.00000  270.000000  270.000000   \n",
              "mean     1.022222  149.677778    0.329630    1.05000    1.585185    0.670370   \n",
              "std      0.997891   23.165717    0.470952    1.14521    0.614390    0.943896   \n",
              "min      0.000000   71.000000    0.000000    0.00000    1.000000    0.000000   \n",
              "25%      0.000000  133.000000    0.000000    0.00000    1.000000    0.000000   \n",
              "50%      2.000000  153.500000    0.000000    0.80000    2.000000    0.000000   \n",
              "75%      2.000000  166.000000    1.000000    1.60000    2.000000    1.000000   \n",
              "max      2.000000  202.000000    1.000000    6.20000    3.000000    3.000000   \n",
              "\n",
              "             thal  \n",
              "count  270.000000  \n",
              "mean     4.696296  \n",
              "std      1.940659  \n",
              "min      3.000000  \n",
              "25%      3.000000  \n",
              "50%      3.000000  \n",
              "75%      7.000000  \n",
              "max      7.000000  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.drop(columns=[\"presence\"]).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSNz-NNOAsIz"
      },
      "source": [
        "At first glance every column semms imporant. The two we might be able to remove are \n",
        "1. sex\n",
        "2. age\n",
        "\n",
        "Age according to this link \n",
        "\n",
        "(https://memorialhermann.org/services/specialties/heart-and-vascular/healthy-living/education/heart-disease-and-age) \n",
        "\n",
        "plays an important role in heart disease.\n",
        "\n",
        "\"Your risk for heart disease increases with age, especially with people of color and for those who are over 65. While the average age for a heart attack is 64.5 for men, and 70.3 for women, nearly 20 percent of those who die of heart disease are under the age of 65.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yudtNwnZBRaX"
      },
      "source": [
        "Also from the same link we can see that the average age for men and women are different but we could not find any indicator that plays an important role for a specific gender from the other columns. Men might have a higher resting blood pressure than women for example but if we do not know if that means that there is a higher chance for heart disease. We cannot remove anything based on these alone so we have to check for correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "e_K8NVdNIj3m",
        "outputId": "a3e15eee-8416-4b7d-f245-b3c3ee754ce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff105e76bd0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.drop(columns=[\"presence\"]).corrwith(df.presence).plot(kind='bar', grid=True, figsize=(20, 8), title=\"Presence Correlation \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx5I-BRpfNqS"
      },
      "source": [
        "The least correlated column is fbs so we can drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Zzu5ECVDcpls"
      },
      "outputs": [],
      "source": [
        "dfx = df.drop(columns=[\"fbs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4LVLLPnATf5"
      },
      "source": [
        "Adding the cost matrix\n",
        "\n",
        "**Cost Matrix**\n",
        "\n",
        "| | absence | presence|\n",
        "|-|-------- |:--------|\n",
        "|absence| 0 | 1|\n",
        "|presence|5|0|\n",
        "\n",
        "_______ \n",
        "\n",
        "> Where the rows represent the true values and the columns the predicted. \n",
        "______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AkOEry9ufTjq"
      },
      "outputs": [],
      "source": [
        "cost_matrix = [[0 , 1], [5, 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tw2ujubQBw6r"
      },
      "outputs": [],
      "source": [
        "y = dfx[\"presence\"].values\n",
        "\n",
        "y[y == 1] = 0\n",
        "y[y == 2] = 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(dfx.drop(columns=[\"presence\"]).values, \n",
        "                                                    dfx[\"presence\"].values, test_size=0.3, \n",
        "                                                    random_state=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2EPctW1DCtgq"
      },
      "outputs": [],
      "source": [
        "names = ['Random Forest', 'Linear SVM','Naive Bayes']\n",
        "classifiers = [RandomForestClassifier(n_estimators=100, random_state=0), \n",
        "               SVC(kernel='linear', C=10),\n",
        "               GaussianNB()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K43q6b7pDtvp",
        "outputId": "122be6d4-ca8c-4b0f-f5fe-7f960a646190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.88      0.79      0.84        48\n",
            "    presence       0.74      0.85      0.79        33\n",
            "\n",
            "    accuracy                           0.81        81\n",
            "   macro avg       0.81      0.82      0.81        81\n",
            "weighted avg       0.82      0.81      0.82        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[38 10]\n",
            " [ 5 28]] \n",
            "\n",
            "loss: 35\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.87      0.83      0.85        48\n",
            "    presence       0.77      0.82      0.79        33\n",
            "\n",
            "    accuracy                           0.83        81\n",
            "   macro avg       0.82      0.83      0.82        81\n",
            "weighted avg       0.83      0.83      0.83        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[40  8]\n",
            " [ 6 27]] \n",
            "\n",
            "loss: 38\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.84      0.79      0.82        48\n",
            "    presence       0.72      0.79      0.75        33\n",
            "\n",
            "    accuracy                           0.79        81\n",
            "   macro avg       0.78      0.79      0.79        81\n",
            "weighted avg       0.79      0.79      0.79        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[38 10]\n",
            " [ 7 26]] \n",
            "\n",
            "loss: 45\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, clf in zip(names, classifiers):\n",
        "  print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "  print(\"------------------------------------------------------\")\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred = clf.predict(X_test)\n",
        "  print(classification_report(y_test, y_pred, target_names=['absence','presence']))\n",
        "\n",
        "  conf_m = confusion_matrix(y_test, y_pred,)\n",
        "  print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "  print(conf_m , \"\\n\") \n",
        "  loss = np.sum(conf_m * cost_matrix)\n",
        "  print(\"loss: %d\" %loss)\n",
        "  print(\"------------------------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEGG2Ex7Y3Xf"
      },
      "source": [
        "We can see that between Random Forest, Linear SVM and Naive Bayes, the lowest cost is represented by the Random Forest algorithm which is 35. It is followed by Linear SVM with cost 38 and finally, as expected, Naive Bayes has the biggest one which is 45. In terms of accuracy, Linear SVM has the highest but we are now focusing only on the cost each algorithm represents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDiqaaXFMMTV"
      },
      "source": [
        "**Calibration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNmecuIwNPUj"
      },
      "source": [
        "Cost matrix required for BayesMinimumRiskClassifier and costcla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "btpMPSYxNk5w"
      },
      "outputs": [],
      "source": [
        "fp = np.full((y_test.shape[0],1), 1)\n",
        "fn = np.full((y_test.shape[0],1), 5)\n",
        "tp = np.zeros((y_test.shape[0],1))\n",
        "tn = np.zeros((y_test.shape[0],1))\n",
        "cost_matrix = np.hstack((fp, fn, tp, tn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDCaj4Mmgw3K"
      },
      "source": [
        "*No calibration*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HvceG0K9w38Z"
      },
      "outputs": [],
      "source": [
        "names = ['Random Forest', 'Linear SVM','Naive Bayes']\n",
        "classifiers = [RandomForestClassifier(n_estimators=100, random_state=0), \n",
        "               SVC(kernel='linear', C=10,probability=True),\n",
        "               GaussianNB()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXB3XVpVwJ48",
        "outputId": "825dd3ee-7c3c-4b56-986b-31e24d90fc7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.29      0.45        48\n",
            "    presence       0.49      1.00      0.66        33\n",
            "\n",
            "    accuracy                           0.58        81\n",
            "   macro avg       0.75      0.65      0.56        81\n",
            "weighted avg       0.79      0.58      0.54        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[14 34]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 34\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.92      0.23      0.37        48\n",
            "    presence       0.46      0.97      0.63        33\n",
            "\n",
            "    accuracy                           0.53        81\n",
            "   macro avg       0.69      0.60      0.50        81\n",
            "weighted avg       0.73      0.53      0.47        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[11 37]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 42\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.86      0.65      0.74        48\n",
            "    presence       0.62      0.85      0.72        33\n",
            "\n",
            "    accuracy                           0.73        81\n",
            "   macro avg       0.74      0.75      0.73        81\n",
            "weighted avg       0.76      0.73      0.73        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[31 17]\n",
            " [ 5 28]] \n",
            "\n",
            "loss: 42\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, clf in zip(names, classifiers):\n",
        "  print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "  print(\"------------------------------------------------------\")\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "  y_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "  bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "\n",
        "  y_pred = bmr.predict(y_pred_proba, cost_matrix)\n",
        "  \n",
        "  print(classification_report(y_test, y_pred, target_names=['absence','presence']))\n",
        "\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "  print(conf_m , \"\\n\") \n",
        "  loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "  print(\"loss: %d\" %loss)\n",
        "  print(\"------------------------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au2CBqoIcrwd"
      },
      "source": [
        "After applying Bayes Minimum Risk Classifier with no callibration, average cost seems to be slightly less but with no significant impact to the model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kivrpS-kg_bF"
      },
      "source": [
        "*costcla calibration on training set*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toHZ8Z5jxyqc",
        "outputId": "2af348bc-0ee4-46b8-c565-51baf1039ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.85      0.85      0.85        48\n",
            "    presence       0.79      0.79      0.79        33\n",
            "\n",
            "    accuracy                           0.83        81\n",
            "   macro avg       0.82      0.82      0.82        81\n",
            "weighted avg       0.83      0.83      0.83        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[41  7]\n",
            " [ 7 26]] \n",
            "\n",
            "loss: 42\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.96      0.48      0.64        48\n",
            "    presence       0.56      0.97      0.71        33\n",
            "\n",
            "    accuracy                           0.68        81\n",
            "   macro avg       0.76      0.72      0.68        81\n",
            "weighted avg       0.80      0.68      0.67        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[23 25]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 30\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.93      0.29      0.44        48\n",
            "    presence       0.48      0.97      0.65        33\n",
            "\n",
            "    accuracy                           0.57        81\n",
            "   macro avg       0.71      0.63      0.55        81\n",
            "weighted avg       0.75      0.57      0.53        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[14 34]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 39\n",
            "------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for name, clf in zip(names, classifiers):\n",
        "  print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "  print(\"------------------------------------------------------\")\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "  train_proba = clf.predict_proba(X_train)\n",
        "\n",
        "  bmr = BayesMinimumRiskClassifier(calibration=True)\n",
        "  bmr.fit(y_train, train_proba) \n",
        "\n",
        "  y_pred_proba = clf.predict_proba(X_test)\n",
        "  y_pred = bmr.predict(y_pred_proba, cost_matrix)\n",
        "\n",
        "  print(classification_report(y_test, y_pred,zero_division=0,target_names=['absence','presence']))\n",
        "\n",
        "  conf_m = confusion_matrix(y_test, y_pred)\n",
        "  print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "  print(conf_m , \"\\n\") \n",
        "  loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "  print(\"loss: %d\" %loss)\n",
        "  print(\"------------------------------------------------------\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBNY1GXlfbZm"
      },
      "source": [
        "After applying Bayes Minimum Risk Classifier with callibration, we see a better performance of the model in terms of cost at L-SVM and Naive Bayes whose cost is now downgrated from 38 to 30 and from 45 to 39 respectively. On the other hand, Random Forest represents  raised cost which went from 35 to 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYlmlWd6f_dU"
      },
      "source": [
        "Sigmoid and Isotonic Calibration with cv 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocmq_jPkyqVF",
        "outputId": "89374b30-2c3c-4b48-d743-8627ce130da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4msigmoid\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.29      0.45        48\n",
            "    presence       0.49      1.00      0.66        33\n",
            "\n",
            "    accuracy                           0.58        81\n",
            "   macro avg       0.75      0.65      0.56        81\n",
            "weighted avg       0.79      0.58      0.54        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[14 34]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 34\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.94      0.35      0.52        48\n",
            "    presence       0.51      0.97      0.67        33\n",
            "\n",
            "    accuracy                           0.60        81\n",
            "   macro avg       0.73      0.66      0.59        81\n",
            "weighted avg       0.77      0.60      0.58        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[17 31]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 36\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.90      0.56      0.69        48\n",
            "    presence       0.59      0.91      0.71        33\n",
            "\n",
            "    accuracy                           0.70        81\n",
            "   macro avg       0.74      0.74      0.70        81\n",
            "weighted avg       0.77      0.70      0.70        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[27 21]\n",
            " [ 3 30]] \n",
            "\n",
            "loss: 36\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n",
            "\u001b[4misotonic\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.94      0.33      0.49        48\n",
            "    presence       0.50      0.97      0.66        33\n",
            "\n",
            "    accuracy                           0.59        81\n",
            "   macro avg       0.72      0.65      0.58        81\n",
            "weighted avg       0.76      0.59      0.56        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[16 32]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 37\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.94      0.35      0.52        48\n",
            "    presence       0.51      0.97      0.67        33\n",
            "\n",
            "    accuracy                           0.60        81\n",
            "   macro avg       0.73      0.66      0.59        81\n",
            "weighted avg       0.77      0.60      0.58        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[17 31]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 36\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.94      0.31      0.47        48\n",
            "    presence       0.49      0.97      0.65        33\n",
            "\n",
            "    accuracy                           0.58        81\n",
            "   macro avg       0.71      0.64      0.56        81\n",
            "weighted avg       0.76      0.58      0.54        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[15 33]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 38\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n"
          ]
        }
      ],
      "source": [
        "calibrations = ['sigmoid','isotonic']\n",
        "\n",
        "for calibration in calibrations:\n",
        "  print(\"\\033[4m\" +  calibration + \"\\033[0m\")\n",
        "  print(\"=======================================================================\")\n",
        "  for name, clf in zip(names, classifiers):\n",
        "    print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "\n",
        "    cc = CalibratedClassifierCV(clf, method=calibration, cv=3)\n",
        "    model = cc.fit(X_train, y_train)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "    y_pred = bmr.predict(y_pred_proba, cost_matrix)\n",
        "\n",
        "    print(classification_report(y_test, y_pred,zero_division=0,target_names=['absence','presence']))\n",
        "\n",
        "    conf_m = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "    print(conf_m , \"\\n\") \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(\"loss: %d\" %loss)\n",
        "    print(\"------------------------------------------------------\\n\")\n",
        "  print(\"=======================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJIpEHfrlPUw"
      },
      "source": [
        "test other cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI3_FAwvlSOF",
        "outputId": "06b8caf9-0003-4b77-edab-42dcb560e340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4msigmoid\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.23      0.37        48\n",
            "    presence       0.47      1.00      0.64        33\n",
            "\n",
            "    accuracy                           0.54        81\n",
            "   macro avg       0.74      0.61      0.51        81\n",
            "weighted avg       0.78      0.54      0.48        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[11 37]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 37\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.89      0.17      0.28        48\n",
            "    presence       0.44      0.97      0.61        33\n",
            "\n",
            "    accuracy                           0.49        81\n",
            "   macro avg       0.67      0.57      0.45        81\n",
            "weighted avg       0.71      0.49      0.41        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[ 8 40]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 45\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.00      0.00      0.00        48\n",
            "    presence       0.41      1.00      0.58        33\n",
            "\n",
            "    accuracy                           0.41        81\n",
            "   macro avg       0.20      0.50      0.29        81\n",
            "weighted avg       0.17      0.41      0.24        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[ 0 48]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 48\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n",
            "\u001b[4misotonic\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.42      0.59        48\n",
            "    presence       0.54      1.00      0.70        33\n",
            "\n",
            "    accuracy                           0.65        81\n",
            "   macro avg       0.77      0.71      0.65        81\n",
            "weighted avg       0.81      0.65      0.63        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[20 28]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 28\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.95      0.42      0.58        48\n",
            "    presence       0.53      0.97      0.69        33\n",
            "\n",
            "    accuracy                           0.64        81\n",
            "   macro avg       0.74      0.69      0.63        81\n",
            "weighted avg       0.78      0.64      0.62        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[20 28]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 33\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.95      0.40      0.56        48\n",
            "    presence       0.52      0.97      0.68        33\n",
            "\n",
            "    accuracy                           0.63        81\n",
            "   macro avg       0.74      0.68      0.62        81\n",
            "weighted avg       0.78      0.63      0.61        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[19 29]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 34\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n"
          ]
        }
      ],
      "source": [
        "calibrations = ['sigmoid','isotonic']\n",
        "\n",
        "for calibration in calibrations:\n",
        "  print(\"\\033[4m\" +  calibration + \"\\033[0m\")\n",
        "  print(\"=======================================================================\")\n",
        "  for name, clf in zip(names, classifiers):\n",
        "    print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "\n",
        "    cc = CalibratedClassifierCV(clf, method=calibration, cv=10)\n",
        "    model = cc.fit(X_train, y_train)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "    y_pred = bmr.predict(y_pred_proba, cost_matrix)\n",
        "\n",
        "    print(classification_report(y_test, y_pred,zero_division=0,target_names=['absence','presence']))\n",
        "\n",
        "    conf_m = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "    print(conf_m , \"\\n\") \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(\"loss: %d\" %loss)\n",
        "    print(\"------------------------------------------------------\\n\")\n",
        "  print(\"=======================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQ2b8fD2lZ9l",
        "outputId": "99bf43e5-2869-4ec4-ba5e-60a1637232a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[4msigmoid\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.19      0.32        48\n",
            "    presence       0.46      1.00      0.63        33\n",
            "\n",
            "    accuracy                           0.52        81\n",
            "   macro avg       0.73      0.59      0.47        81\n",
            "weighted avg       0.78      0.52      0.44        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[ 9 39]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 39\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.86      0.12      0.22        48\n",
            "    presence       0.43      0.97      0.60        33\n",
            "\n",
            "    accuracy                           0.47        81\n",
            "   macro avg       0.64      0.55      0.41        81\n",
            "weighted avg       0.68      0.47      0.37        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[ 6 42]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 47\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.00      0.00      0.00        48\n",
            "    presence       0.41      1.00      0.58        33\n",
            "\n",
            "    accuracy                           0.41        81\n",
            "   macro avg       0.20      0.50      0.29        81\n",
            "weighted avg       0.17      0.41      0.24        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[ 0 48]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 48\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n",
            "\u001b[4misotonic\u001b[0m\n",
            "=======================================================================\n",
            "\u001b[4mRandom Forest\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       1.00      0.44      0.61        48\n",
            "    presence       0.55      1.00      0.71        33\n",
            "\n",
            "    accuracy                           0.67        81\n",
            "   macro avg       0.78      0.72      0.66        81\n",
            "weighted avg       0.82      0.67      0.65        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[21 27]\n",
            " [ 0 33]] \n",
            "\n",
            "loss: 27\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mLinear SVM\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.95      0.44      0.60        48\n",
            "    presence       0.54      0.97      0.70        33\n",
            "\n",
            "    accuracy                           0.65        81\n",
            "   macro avg       0.75      0.70      0.65        81\n",
            "weighted avg       0.79      0.65      0.64        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[21 27]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 32\n",
            "------------------------------------------------------\n",
            "\n",
            "\u001b[4mNaive Bayes\u001b[0m\n",
            "------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     absence       0.95      0.44      0.60        48\n",
            "    presence       0.54      0.97      0.70        33\n",
            "\n",
            "    accuracy                           0.65        81\n",
            "   macro avg       0.75      0.70      0.65        81\n",
            "weighted avg       0.79      0.65      0.64        81\n",
            "\n",
            "\u001b[4mConfusion matrix\u001b[0m\n",
            "[[21 27]\n",
            " [ 1 32]] \n",
            "\n",
            "loss: 32\n",
            "------------------------------------------------------\n",
            "\n",
            "=======================================================================\n"
          ]
        }
      ],
      "source": [
        "calibrations = ['sigmoid','isotonic']\n",
        "\n",
        "for calibration in calibrations:\n",
        "  print(\"\\033[4m\" +  calibration + \"\\033[0m\")\n",
        "  print(\"=======================================================================\")\n",
        "  for name, clf in zip(names, classifiers):\n",
        "    print(\"\\033[4m\" +  name + \"\\033[0m\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "\n",
        "    cc = CalibratedClassifierCV(clf, method=calibration, cv=11)\n",
        "    model = cc.fit(X_train, y_train)\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "    bmr = BayesMinimumRiskClassifier(calibration=False)\n",
        "    y_pred = bmr.predict(y_pred_proba, cost_matrix)\n",
        "\n",
        "    print(classification_report(y_test, y_pred,zero_division=0,target_names=['absence','presence']))\n",
        "\n",
        "    conf_m = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\033[4m\" + \"Confusion matrix\" + \"\\033[0m\")\n",
        "    print(conf_m , \"\\n\") \n",
        "    loss = cost_loss(y_test, y_pred, cost_matrix)\n",
        "    print(\"loss: %d\" %loss)\n",
        "    print(\"------------------------------------------------------\\n\")\n",
        "  print(\"=======================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8JKEk3ppUui"
      },
      "source": [
        "After apllying Isotonic callibration, the results are overall better. The sigmoid did not yield good results for some reason. One reason might be that the dataset is small.\n",
        "\n",
        "Also note the with the Linear SVM the sigmoid callibration is worse than the other models, which is strange because the sigmoid calibration tends to work well with Linear SVMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QguitkV3oF_h"
      },
      "source": [
        "**The first part of the project continues on the project-A2.ipynb**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "project1-A1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
